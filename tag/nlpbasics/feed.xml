<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator>
  <link href="http://localhost:4000/tag/nlpbasics/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2020-11-27T23:08:38+09:00</updated>
  <id>http://localhost:4000/tag/nlpbasics/feed.xml</id>

  
  
  

  
    <title type="html">DL&amp;amp;NLP | </title>
  

  
    <subtitle>딥러닝과 NLP 논문 공부, 구현 블로그</subtitle>
  

  

  
    
      
    
  

  
  

  
    <entry>
      <title type="html">자연어 처리 기본 - 콜세라 강의 1주차 1리뷰</title>
      <link href="http://localhost:4000/nlp_coursera_seq_week1_1" rel="alternate" type="text/html" title="자연어 처리 기본 - 콜세라 강의 1주차 1리뷰" />
      <published>1863-11-28T00:10:00+09:00</published>
      <updated>1863-11-28T00:10:00+09:00</updated>
      <id>http://localhost:4000/nlp_coursera_seq_week1_1</id>
      <content type="html" xml:base="http://localhost:4000/nlp_coursera_seq_week1_1">&lt;h2 id=&quot;배경-지식이-있는-상태에서-수업을-들으면-더-잘-이해가-되지-않을까-싶어-요약본을-만들어-봤습니다&quot;&gt;배경 지식이 있는 상태에서 수업을 들으면 더 잘 이해가 되지 않을까 싶어 요약본을 만들어 봤습니다.&lt;/h2&gt;

&lt;h2 id=&quot;강의-특징에-따라--이-강의에선-무엇을-배웁니다-만-설명할-수도-혹은-해당-강의-내용-일부분전체를-설명할-수도-있습니다&quot;&gt;강의 특징에 따라 ‘ 이 강의에선 무엇을 배웁니다’ 만 설명할 수도, 혹은 해당 강의 내용 일부분/전체를 설명할 수도 있습니다.&lt;/h2&gt;

&lt;p&gt;회원가입만 하면 coursera의 deeplearning.ai 관련 강의 영상을 전부 무료로 볼 수 있습니다.   &lt;br /&gt;
(하지만 유료 구독을 하고 매 주 코딩 과제를 하시길 추천 드립니다.)     &lt;br /&gt;
강의 주소 : &lt;a href=&quot;https://www.coursera.org/learn/nlp-sequence-models&quot;&gt;링크&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;1why-sequence-models시퀀스-모델을-사용하는-이유&quot;&gt;1.Why sequence models(시퀀스 모델을 사용하는 이유)&lt;/h1&gt;
&lt;p&gt;시퀀스 모델의 파라미터(W,b 값들)는 다른 모델 구조와는 다르게 시간 순서에 따른 정보까지 가집니다.      &lt;br /&gt;
시퀀스 모델은 입력 or 출력 데이터가 ‘순서’ 를 가질 경우에 유용합니다.     &lt;br /&gt;
ex) 음성 데이터(파동의 순서), 텍스트 데이터(문장 구성 토큰의 순서), 동영상 데이터(이미지의 순서), DNA 염기서열(아미노산 순서) 등등..&lt;/p&gt;

&lt;h1 id=&quot;2notation시퀀스-모델-강의에서-사용할-용어-정리&quot;&gt;2.Notation(시퀀스 모델 강의에서 사용할 용어 정리)&lt;/h1&gt;
&lt;p&gt;입력 시퀀스 x 가 9 단어로 이뤄진 문장 ‘Harry Potter and Hermione Granger invented a new spell’ 일 때,&lt;/p&gt;

&lt;p&gt;x&lt;sup&gt;(i) &amp;lt;t&amp;gt; &lt;/sup&gt; 는 i번 째 입력 시퀀스 데이터의 t 번 째 요소입니다. (t는 time step)     &lt;br /&gt;
T&lt;sup&gt;(i)&lt;/sup&gt;&lt;sub&gt;x&lt;/sub&gt; = 9 는 i번 째 입력 시퀀스 데이터의 요소 수(time step 수)는 9 라는 뜻입니다.&lt;/p&gt;

&lt;p&gt;c.f.) Y(출력값 혹은 라벨값) 이 시퀀스 데이터일 경우에도 마찬가지로 사용합니다. (위 notation에서 x만 y로 바꾸면 되겠죠.)&lt;/p&gt;

&lt;p&gt;텍스트 데이터를 다룰 땐 아래 그림과 같이 사전(vocabulary)을 만들어 이용합니다.(이 경우는 단어 단위 사전)       &lt;br /&gt;
사전에 들어 갈 요소는 ‘음소’ , ‘어절’, ‘음절’ 등 여러 종류의 단위가 가능하며, 이를 토큰 이라고 합니다.    &lt;br /&gt;
(왠만하면 앞으로는 토큰 이라고 얘기 하겠습니다.)    &lt;br /&gt;
&lt;img src=&quot;assets/built/images/coursera_week1_2_vocabulary.png&quot; alt=&quot;&quot; /&gt;
텍스트 데이터는 시퀀스 데이터 중 하나입니다.&lt;/p&gt;

&lt;p&gt;텍스트는 string 값이지만 컴퓨터는 string 보단 숫자를 더 잘 이해할 수 있기 때문에 숫자로 치환해 줍니다.&lt;/p&gt;

&lt;p&gt;자연어 처리에서 string -&amp;gt; 숫자 로 바꿀 때, ‘벡터’ 로 표현하며, ‘one-hot 벡터 인코딩 방법’ 과 ‘단어 embedding 방법’을 사용합니다.&lt;/p&gt;

&lt;p&gt;one-hot 벡터 인코딩은 하나의 요소만 1이고 나머지는 0인 벡터로 인코딩(or 치환) 하는 방법 입니다.&lt;/p&gt;

&lt;p&gt;위 그림에서, vocabulary의 1번째 토큰은 “a” 이며, 1번째 토큰을 one-hot 인코딩 하면     &lt;br /&gt;
[1 0 0 ….. 0] 이 됩니다.(1 번째 원소만 1값)&lt;/p&gt;

&lt;p&gt;벡터의 크기는 사전의 크기와 동일합니다.(vocabulary 가 100개 토큰으로 이뤄져 있다면 벡터 크기는 1x100)   &lt;br /&gt;
(c.f. 벡터 index 시작은 0이지만 위 설명에서만 1이라고 생각합시다 ㅎㅎ)&lt;/p&gt;

&lt;p&gt;그렇다면 vocabulary의 i 번째 토큰을 one-hot 인코딩 하면   &lt;br /&gt;
[0 . . … 1 . . …. 0] 인 i 번째 요소만 1인 벡터가 됩니다.&lt;/p&gt;

&lt;p&gt;( c.f. vocabulary에 없는 토큰은 vocabulary에 “unk”(==unknown) 이라는 토큰을 추가해 줌으로써 해결합니다.   &lt;br /&gt;
vocabulary에 없는 토큰은 전부 “unk” 토큰으로 처리하는 거죠.   &lt;br /&gt;
이러한 토큰을 NLP 영역(domain)에선 “스페셜 토큰” 이라고 하며,     &lt;br /&gt;
UNK(Unknwon), SEP(seperateor), PAD(padding), SOS(Start of Sentence), EOS(End of Sentence) 등이 있습니다.  )&lt;/p&gt;

&lt;h1 id=&quot;3recurrent-neural-network-modelrnn-순환-신경망-모델&quot;&gt;3.Recurrent Neural Network Model(RNN, 순환 신경망 모델)&lt;/h1&gt;
&lt;p&gt;이 강의에선 순서가 있는 데이터를 처리할 때 standard network 사용의 단점과, 이를 개선한 RNN을 소개합니다.&lt;/p&gt;

&lt;p&gt;(아래 그림을 봐주세요)
RNN 모델은 Cell 로 이뤄져 있습니다. 아래 그림에서 좌/우의 RNN 모델은 같은 모델이며 Cell 하나로 이뤄진 모델입니다.     &lt;br /&gt;
Cell은 이전 time step의 활성화 값(activation a&lt;sup&gt;&amp;lt;t-1&amp;gt;&lt;/sup&gt;)과 현재 time step의 입력 데이터 x&lt;sup&gt;&amp;lt;t&amp;gt;&lt;/sup&gt;    &lt;br /&gt;
를 입력받아 파라미터(Waa, Wax, Wya)값과 연산해 다음 활성화 값 a&lt;sup&gt;&amp;lt;t&amp;gt;&lt;/sup&gt; 과 현재 time step의 출력값 y^&lt;sup&gt;&amp;lt;t&amp;gt;&lt;/sup&gt;      &lt;br /&gt;
를 출력합니다.&lt;/p&gt;

&lt;p&gt;아래 그림의 경우에선 T&lt;sub&gt;x&lt;/sub&gt; 번의 연산 동안 동일한 가중치 Waa, Wax, Wya으로 T&lt;sub&gt;x&lt;/sub&gt; 개의 출력값(y^), 활성화 값(a) 를 계산할 것입니다.&lt;/p&gt;

&lt;p&gt;Waa, Wax, Wya는 하나 씩 있지만 back propagation 을 위한 cache는 종류 별로 T&lt;sub&gt;x&lt;/sub&gt; 개 만큼 생기겠죠?
&lt;img src=&quot;assets/built/images/coursera_week1_3_rnn.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(아래 그림을 봐주세요)    &lt;br /&gt;
time step t 에선 a&lt;sup&gt;&amp;lt;t-1&amp;gt;&lt;/sup&gt;와 Waa, x&lt;sup&gt;&amp;lt;t&amp;gt;&lt;/sup&gt;와 Wax 의 곱을 각각 해서 현재 time step 활성화 값 a&lt;sup&gt;&amp;lt;t&amp;gt;&lt;/sup&gt;를 구합니다.      &lt;br /&gt;
또한, 두 연산은 Waa, Wax를 연결(concatenate)하고 a&lt;sup&gt;&amp;lt;t-1&amp;gt;&lt;/sup&gt;와 x&lt;sup&gt;&amp;lt;t&amp;gt;&lt;/sup&gt;를 연결한 행렬을 곱함으로써 연산 횟수를 줄일 수가 있죠.        &lt;br /&gt;
&lt;img src=&quot;assets/built/images/coursera_week1_3_calculate.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;4backpropagation-through-time&quot;&gt;4.Backpropagation through time&lt;/h1&gt;

&lt;h1 id=&quot;5different-types-of-rnns&quot;&gt;5.Different types of RNNs&lt;/h1&gt;

&lt;h1 id=&quot;6language-model-and-sequence-generation&quot;&gt;6.Language model and sequence generation&lt;/h1&gt;

&lt;h1 id=&quot;7sampling-novel-sequences&quot;&gt;7.Sampling novel sequences&lt;/h1&gt;

&lt;h1 id=&quot;8vanishing-gradients-with-rnns&quot;&gt;8.Vanishing gradients with RNNs&lt;/h1&gt;

&lt;h1 id=&quot;9gated-recurrent-unit-gru&quot;&gt;9.Gated Recurrent Unit (GRU)&lt;/h1&gt;

&lt;h1 id=&quot;10long-short-term-memory-lstm&quot;&gt;10.Long Short Term Memory (LSTM)&lt;/h1&gt;

&lt;h1 id=&quot;11bidirectional-rnn&quot;&gt;11.Bidirectional RNN&lt;/h1&gt;

&lt;h1 id=&quot;12deep-rnns&quot;&gt;12.Deep RNNs&lt;/h1&gt;</content>

      
      
      
      
      

      <author>
          <name>DongHee Youn</name>
        
        
      </author>

      

      
        <category term="NlpBasics" />
      

      
        <summary type="html">배경 지식이 있는 상태에서 수업을 들으면 더 잘 이해가 되지 않을까 싶어 요약본을 만들어 봤습니다.</summary>
      

      
      
    </entry>
  
</feed>
